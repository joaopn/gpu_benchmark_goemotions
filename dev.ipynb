{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests the precision of the ONNX and ONNX-FP16 models compared to the base one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "from run_benchmark import sentiment_analysis_batched\n",
    "import pandas as pd\n",
    "%load_ext line_profiler\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "model_id_onnx = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
    "file_name_onnx = \"onnx/model.onnx\"\n",
    "\n",
    "model_id_onnx_fp16 = \"joaopn/roberta-base-go_emotions-onnx-fp16\"\n",
    "file_name_onnx_fp16 = \"model.onnx\"\n",
    "\n",
    "df = pd.read_csv('data/random_sample_10k.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:15<00:00, 82.48it/s]\n",
      "c:\\Users\\Joao\\miniconda3\\envs\\bench\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The ONNX file onnx/model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "(Batch size 8): 100%|██████████| 1250/1250 [00:10<00:00, 123.53it/s]\n",
      "c:\\Users\\Joao\\miniconda3\\envs\\bench\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "(Batch size 8): 100%|██████████| 1250/1250 [00:04<00:00, 255.33it/s]\n"
     ]
    }
   ],
   "source": [
    "field = 'body'\n",
    "batch_size = 8\n",
    "gpu_id = 0\n",
    "\n",
    "model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "model_id_onnx = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
    "file_name_onnx = \"onnx/model.onnx\"\n",
    "\n",
    "model_id_onnx_fp16 = \"joaopn/roberta-base-go_emotions-onnx-fp16\"\n",
    "file_name_onnx_fp16 = \"model.onnx\"\n",
    "\n",
    "_,_,df_test1b = sentiment_analysis_batched(df, 'gpu', 'torch', model_id, batch_size, field, file_name = None, gpu_id=0)\n",
    "_,_,df_test2b = sentiment_analysis_batched(df, 'gpu', 'onnx', model_id_onnx, batch_size, field, file_name = file_name_onnx, gpu_id=0)\n",
    "_,_,df_test3b = sentiment_analysis_batched(df, 'gpu', 'onnx', model_id_onnx_fp16, batch_size, field, file_name = file_name_onnx_fp16, gpu_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:17<00:00, 69.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 18.7622 s\n",
      "File: c:\\Users\\Joao\\repo\\gpu_benchmark_goemotions\\run_benchmark.py\n",
      "Function: sentiment_analysis_batched at line 56\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    56                                           def sentiment_analysis_batched(model_id, df, field_name, batch_size, gpu_id):\n",
      "    57         1          0.3      0.3      1.4      tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
      "    58         1          0.4      0.4      2.2      model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
      "    59         1          0.0      0.0      0.0      device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
      "    60         1          0.1      0.1      0.5      model.to(device)\n",
      "    61                                           \n",
      "    62         1          0.0      0.0      0.0      start_time = time.time()\n",
      "    63         1          0.0      0.0      0.0      results = []\n",
      "    64                                           \n",
      "    65                                               # Precompute id2label mapping\n",
      "    66         1          0.0      0.0      0.0      id2label = model.config.id2label\n",
      "    67                                           \n",
      "    68      1251          0.1      0.0      0.5      for start_idx in tqdm(range(0, len(df), batch_size), desc=f\"(Batch size {batch_size})\"):\n",
      "    69      1250          0.0      0.0      0.0          end_idx = start_idx + batch_size\n",
      "    70      1250          0.1      0.0      0.5          texts = df[field_name].iloc[start_idx:end_idx].tolist()\n",
      "    71                                           \n",
      "    72      1250          1.3      0.0      6.8          inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
      "    73      1250          3.9      0.0     20.5          input_ids = inputs['input_ids'].to(device)\n",
      "    74      1250          0.0      0.0      0.2          attention_mask = inputs['attention_mask'].to(device)\n",
      "    75                                           \n",
      "    76      2500          0.0      0.0      0.1          with torch.no_grad():\n",
      "    77      1250         12.6      0.0     67.1              outputs = model(input_ids, attention_mask=attention_mask)\n",
      "    78      1250          0.0      0.0      0.1          predictions = torch.sigmoid(outputs.logits)  # Use sigmoid for multi-label classification\n",
      "    79                                           \n",
      "    80                                                   # Collect predictions on GPU\n",
      "    81      1250          0.0      0.0      0.0          results.append(predictions)\n",
      "    82                                           \n",
      "    83                                               # Concatenate all results on GPU\n",
      "    84         1          0.0      0.0      0.0      all_predictions = torch.cat(results, dim=0).cpu().numpy()\n",
      "    85                                           \n",
      "    86                                               # Convert to DataFrame\n",
      "    87         1          0.0      0.0      0.0      predictions_df = pd.DataFrame(all_predictions, columns=[id2label[i] for i in range(all_predictions.shape[1])])\n",
      "    88                                           \n",
      "    89                                               # Add prediction columns to the original DataFrame\n",
      "    90         1          0.0      0.0      0.0      combined_df = pd.concat([df.reset_index(drop=True), predictions_df], axis=1)\n",
      "    91                                           \n",
      "    92         1          0.0      0.0      0.0      elapsed_time = time.time() - start_time\n",
      "    93         1          0.0      0.0      0.0      messages_per_second = len(df) / elapsed_time\n",
      "    94                                           \n",
      "    95         1          0.0      0.0      0.0      return elapsed_time, messages_per_second, combined_df"
     ]
    }
   ],
   "source": [
    "field = 'body'\n",
    "batch_size = 8\n",
    "%lprun -u 1 -f sentiment_analysis_batched sentiment_analysis_batched(model_id, df, field,batch_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:16<00:00, 73.64it/s]\n"
     ]
    }
   ],
   "source": [
    "_,_,df3 = sentiment_analysis_batched(model_id, df, field,batch_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>caring</th>\n",
       "      <th>confusion</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>desire</th>\n",
       "      <th>disappointment</th>\n",
       "      <th>...</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.045760</td>\n",
       "      <td>0.147581</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>0.033266</td>\n",
       "      <td>0.028019</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.026043</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.621454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.008919</td>\n",
       "      <td>0.028587</td>\n",
       "      <td>0.003058</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.003179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.010806</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.924112</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.011870</td>\n",
       "      <td>0.020030</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.256123</td>\n",
       "      <td>0.464083</td>\n",
       "      <td>0.001105</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.006044</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.500028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003837</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.017059</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.003035</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.952363</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.001808</td>\n",
       "      <td>0.001421</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.020517</td>\n",
       "      <td>0.002736</td>\n",
       "      <td>0.409891</td>\n",
       "      <td>0.521960</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.013635</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.015158</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>0.008087</td>\n",
       "      <td>0.287632</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005196</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.132416</td>\n",
       "      <td>0.037693</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>0.047171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.020429</td>\n",
       "      <td>0.006217</td>\n",
       "      <td>0.036360</td>\n",
       "      <td>0.056457</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>0.021115</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.403003</td>\n",
       "      <td>9995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.004345</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.008969</td>\n",
       "      <td>0.010784</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.006598</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.002911</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>0.951456</td>\n",
       "      <td>9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.227706</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.034716</td>\n",
       "      <td>0.002264</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.004616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.010513</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.119277</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.661385</td>\n",
       "      <td>9997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.007561</td>\n",
       "      <td>0.891062</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.015509</td>\n",
       "      <td>0.010253</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.014622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.035765</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.008558</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.054480</td>\n",
       "      <td>9998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.005681</td>\n",
       "      <td>0.042083</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.005621</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.018196</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.940983</td>\n",
       "      <td>9999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    admiration  amusement     anger  annoyance  approval    caring  confusion  \\\n",
       "0     0.001547   0.000389  0.002694   0.045760  0.147581  0.008183   0.033266   \n",
       "1     0.001154   0.000910  0.001121   0.008919  0.028587  0.003058   0.030400   \n",
       "2     0.001994   0.001221  0.002787   0.011870  0.020030  0.001793   0.256123   \n",
       "3     0.003837   0.001673  0.002111   0.011314  0.017059  0.001365   0.003035   \n",
       "4     0.001439   0.001808  0.001421   0.005099  0.020517  0.002736   0.409891   \n",
       "..         ...        ...       ...        ...       ...       ...        ...   \n",
       "3     0.005196   0.002714  0.002323   0.035727  0.132416  0.037693   0.001585   \n",
       "4     0.003018   0.004345  0.005228   0.008969  0.010784  0.001430   0.001478   \n",
       "5     0.003253   0.001277  0.000587   0.005238  0.227706  0.000934   0.034716   \n",
       "6     0.007561   0.891062  0.001462   0.015509  0.010253  0.000826   0.008041   \n",
       "7     0.002259   0.001216  0.000948   0.005681  0.042083  0.000961   0.004694   \n",
       "\n",
       "    curiosity    desire  disappointment  ...  nervousness  optimism     pride  \\\n",
       "0    0.028019  0.002859        0.024921  ...     0.000842  0.014535  0.000205   \n",
       "1    0.004150  0.002700        0.003179  ...     0.000338  0.010806  0.000190   \n",
       "2    0.464083  0.001105        0.002640  ...     0.000399  0.006044  0.000079   \n",
       "3    0.004966  0.000893        0.002489  ...     0.000172  0.002373  0.000197   \n",
       "4    0.521960  0.002616        0.002120  ...     0.001180  0.013635  0.000098   \n",
       "..        ...       ...             ...  ...          ...       ...       ...   \n",
       "3    0.000487  0.006046        0.047171  ...     0.004128  0.020429  0.006217   \n",
       "4    0.001477  0.001170        0.002646  ...     0.000870  0.001874  0.000868   \n",
       "5    0.002264  0.001344        0.004616  ...     0.000468  0.010513  0.000466   \n",
       "6    0.008290  0.001444        0.014622  ...     0.001107  0.003611  0.000415   \n",
       "7    0.001285  0.001343        0.002640  ...     0.000214  0.005621  0.000259   \n",
       "\n",
       "    realization    relief   remorse   sadness  surprise   neutral  index  \n",
       "0      0.026043  0.000712  0.000755  0.002815  0.000929  0.621454      0  \n",
       "1      0.017915  0.000353  0.000401  0.000934  0.000474  0.924112      1  \n",
       "2      0.008397  0.000168  0.000372  0.001185  0.005477  0.500028      2  \n",
       "3      0.004484  0.000212  0.000186  0.000936  0.000574  0.952363      3  \n",
       "4      0.015158  0.000346  0.000803  0.001660  0.008087  0.287632      4  \n",
       "..          ...       ...       ...       ...       ...       ...    ...  \n",
       "3      0.036360  0.056457  0.001847  0.021115  0.000661  0.403003   9995  \n",
       "4      0.006598  0.000678  0.000445  0.002911  0.002966  0.951456   9996  \n",
       "5      0.119277  0.000887  0.000623  0.001027  0.001024  0.661385   9997  \n",
       "6      0.035765  0.001419  0.001488  0.008558  0.008100  0.054480   9998  \n",
       "7      0.018196  0.000404  0.000295  0.000902  0.000602  0.940983   9999  \n",
       "\n",
       "[10000 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_batched2(model_id, df, field_name, batch_size, gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    # Precompute id2label mapping\n",
    "    id2label = model.config.id2label\n",
    "\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=f\"(Batch size {batch_size})\"):\n",
    "        end_idx = start_idx + batch_size\n",
    "        texts = df[field_name].iloc[start_idx:end_idx].tolist()\n",
    "\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.nn.functional.sigmoid(outputs.logits)  # Use sigmoid for multi-label classification\n",
    "\n",
    "        # Collect predictions on GPU\n",
    "        results.append(predictions)\n",
    "\n",
    "    # Concatenate all results on GPU\n",
    "    all_predictions = torch.cat(results, dim=0).cpu().numpy()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_predictions, columns=[id2label[i] for i in range(all_predictions.shape[1])])\n",
    "    results_df['index'] = range(len(df))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    messages_per_second = len(df) / elapsed_time\n",
    "\n",
    "    return elapsed_time, messages_per_second, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_batched_test(model_type, model_id, df, field_name, batch_size, gpu_id, file_name=None):\n",
    "\n",
    "    if model_type == 'torch':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "        device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "\n",
    "    elif model_type == 'onnx':\n",
    "        model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name, provider=\"CUDAExecutionProvider\", provider_options={'device_id': gpu_id})\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    # Precompute id2label mapping\n",
    "    id2label = model.config.id2label\n",
    "\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=f\"(Batch size {batch_size})\"):\n",
    "        end_idx = start_idx + batch_size\n",
    "        texts = df[field_name].iloc[start_idx:end_idx].tolist()\n",
    "\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.nn.functional.sigmoid(outputs.logits)  # Use sigmoid for multi-label classification\n",
    "\n",
    "        # Collect predictions on GPU\n",
    "        results.append(predictions)\n",
    "\n",
    "    # Concatenate all results on GPU\n",
    "    all_predictions = torch.cat(results, dim=0).cpu().numpy()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(all_predictions, columns=[id2label[i] for i in range(all_predictions.shape[1])])\n",
    "\n",
    "    # Add prediction columns to the original DataFrame\n",
    "    combined_df = pd.concat([df.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    messages_per_second = len(df) / elapsed_time\n",
    "\n",
    "    return elapsed_time, messages_per_second, combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:15<00:00, 82.71it/s]\n",
      "c:\\Users\\Joao\\miniconda3\\envs\\bench\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The ONNX file onnx/model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "(Batch size 8): 100%|██████████| 1250/1250 [00:10<00:00, 122.08it/s]\n",
      "c:\\Users\\Joao\\miniconda3\\envs\\bench\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "(Batch size 8): 100%|██████████| 1250/1250 [00:04<00:00, 250.15it/s]\n"
     ]
    }
   ],
   "source": [
    "field = 'body'\n",
    "batch_size = 8\n",
    "gpu_id = 0\n",
    "\n",
    "model_id = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "model_id_onnx = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
    "file_name_onnx = \"onnx/model.onnx\"\n",
    "\n",
    "model_id_onnx_fp16 = \"joaopn/roberta-base-go_emotions-onnx-fp16\"\n",
    "file_name_onnx_fp16 = \"model.onnx\"\n",
    "\n",
    "_,_,df_test1 = sentiment_analysis_batched_test('torch', model_id, df, field,batch_size, 0)\n",
    "_,_,df_test2 = sentiment_analysis_batched_test('onnx', model_id_onnx, df, field,batch_size, 0, file_name_onnx)\n",
    "_,_,df_test3 = sentiment_analysis_batched_test('onnx', model_id_onnx_fp16, df, field,batch_size, 0, file_name_onnx_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis_batched3(model_id, df, field_name, batch_size, gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "    device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    results = []\n",
    "\n",
    "    # Precompute id2label mapping\n",
    "    id2label = model.config.id2label\n",
    "\n",
    "    for start_idx in tqdm(range(0, len(df), batch_size), desc=f\"(Batch size {batch_size})\"):\n",
    "        end_idx = start_idx + batch_size\n",
    "        texts = df[field_name].iloc[start_idx:end_idx].tolist()\n",
    "\n",
    "        inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.nn.functional.sigmoid(outputs.logits)  # Use sigmoid for multi-label classification\n",
    "\n",
    "        # Collect predictions on GPU\n",
    "        results.append(predictions)\n",
    "\n",
    "    # Concatenate all results on GPU\n",
    "    all_predictions = torch.cat(results, dim=0).cpu().numpy()\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    predictions_df = pd.DataFrame(all_predictions, columns=[id2label[i] for i in range(all_predictions.shape[1])])\n",
    "\n",
    "    # Add prediction columns to the original DataFrame\n",
    "    combined_df = pd.concat([df.reset_index(drop=True), predictions_df], axis=1)\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    messages_per_second = len(df) / elapsed_time\n",
    "\n",
    "    return elapsed_time, messages_per_second, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joao\\miniconda3\\envs\\bench\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "The ONNX file onnx/model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "(Batch size 8): 100%|██████████| 1250/1250 [00:10<00:00, 114.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 13.0405 s\n",
      "File: C:\\Users\\Joao\\AppData\\Local\\Temp\\ipykernel_19972\\155550149.py\n",
      "Function: sentiment_analysis_batched_test at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def sentiment_analysis_batched_test(model_type, model_id, df, field_name, batch_size, gpu_id, file_name=None):\n",
      "     2                                           \n",
      "     3         1          0.0      0.0      0.0      if model_type == 'torch':\n",
      "     4                                                   tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
      "     5                                                   model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
      "     6                                                   device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
      "     7                                                   model.to(device)\n",
      "     8                                           \n",
      "     9         1          0.0      0.0      0.0      elif model_type == 'onnx':\n",
      "    10         1          1.9      1.9     14.8          model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name, provider=\"CUDAExecutionProvider\", provider_options={'device_id': gpu_id})\n",
      "    11         1          0.2      0.2      1.4          tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "    12         1          0.0      0.0      0.0          device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
      "    13                                           \n",
      "    14         1          0.0      0.0      0.0      start_time = time.time()\n",
      "    15         1          0.0      0.0      0.0      results = []\n",
      "    16                                           \n",
      "    17                                               # Precompute id2label mapping\n",
      "    18         1          0.0      0.0      0.0      id2label = model.config.id2label\n",
      "    19                                           \n",
      "    20      1251          0.1      0.0      0.4      for start_idx in tqdm(range(0, len(df), batch_size), desc=f\"(Batch size {batch_size})\"):\n",
      "    21      1250          0.0      0.0      0.0          end_idx = start_idx + batch_size\n",
      "    22      1250          0.1      0.0      0.7          texts = df[field_name].iloc[start_idx:end_idx].tolist()\n",
      "    23                                           \n",
      "    24      1250          1.2      0.0      9.2          inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
      "    25      1250          0.1      0.0      0.7          input_ids = inputs['input_ids'].to(device)\n",
      "    26      1250          0.0      0.0      0.3          attention_mask = inputs['attention_mask'].to(device)\n",
      "    27                                           \n",
      "    28      2500          0.0      0.0      0.1          with torch.no_grad():\n",
      "    29      1250          9.4      0.0     72.0              outputs = model(input_ids, attention_mask=attention_mask)\n",
      "    30      1250          0.0      0.0      0.3          predictions = torch.nn.functional.sigmoid(outputs.logits)  # Use sigmoid for multi-label classification\n",
      "    31                                           \n",
      "    32                                                   # Collect predictions on GPU\n",
      "    33      1250          0.0      0.0      0.0          results.append(predictions)\n",
      "    34                                           \n",
      "    35                                               # Concatenate all results on GPU\n",
      "    36         1          0.0      0.0      0.0      all_predictions = torch.cat(results, dim=0).cpu().numpy()\n",
      "    37                                           \n",
      "    38                                               # Convert to DataFrame\n",
      "    39         1          0.0      0.0      0.0      predictions_df = pd.DataFrame(all_predictions, columns=[id2label[i] for i in range(all_predictions.shape[1])])\n",
      "    40                                           \n",
      "    41                                               # Add prediction columns to the original DataFrame\n",
      "    42         1          0.0      0.0      0.0      combined_df = pd.concat([df.reset_index(drop=True), predictions_df], axis=1)\n",
      "    43                                           \n",
      "    44         1          0.0      0.0      0.0      elapsed_time = time.time() - start_time\n",
      "    45         1          0.0      0.0      0.0      messages_per_second = len(df) / elapsed_time\n",
      "    46                                           \n",
      "    47         1          0.0      0.0      0.0      return elapsed_time, messages_per_second, combined_df"
     ]
    }
   ],
   "source": [
    "model_type = 'onnx'\n",
    "%lprun -u 1 -f sentiment_analysis_batched_test sentiment_analysis_batched_test(model_type, model_id_onnx, df, field,batch_size, 0, file_name_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:17<00:00, 69.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 18.8815 s\n",
      "File: C:\\Users\\Joao\\AppData\\Local\\Temp\\ipykernel_60264\\3222582611.py\n",
      "Function: sentiment_analysis_batched2 at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def sentiment_analysis_batched2(model_id, df, field_name, batch_size, gpu_id):\n",
      "     2         1          0.4      0.4      2.0      tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
      "     3         1          0.5      0.5      2.8      model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
      "     4         1          0.0      0.0      0.0      device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
      "     5         1          0.0      0.0      0.2      model.to(device)\n",
      "     6                                           \n",
      "     7         1          0.0      0.0      0.0      start_time = time.time()\n",
      "     8         1          0.0      0.0      0.0      results = []\n",
      "     9                                           \n",
      "    10                                               # Precompute id2label mapping\n",
      "    11         1          0.0      0.0      0.0      id2label = model.config.id2label\n",
      "    12                                           \n",
      "    13      1251          0.1      0.0      0.6      for start_idx in tqdm(range(0, len(df), batch_size), desc=f\"(Batch size {batch_size})\"):\n",
      "    14      1250          0.0      0.0      0.0          end_idx = start_idx + batch_size\n",
      "    15      1250          0.1      0.0      0.6          texts = df[field_name].iloc[start_idx:end_idx].tolist()\n",
      "    16                                           \n",
      "    17      1250          1.3      0.0      6.8          inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
      "    18      1250          3.9      0.0     20.8          input_ids = inputs['input_ids'].to(device)\n",
      "    19      1250          0.1      0.0      0.3          attention_mask = inputs['attention_mask'].to(device)\n",
      "    20                                           \n",
      "    21      2500          0.0      0.0      0.1          with torch.no_grad():\n",
      "    22      1250         12.4      0.0     65.6              outputs = model(input_ids, attention_mask=attention_mask)\n",
      "    23      1250          0.0      0.0      0.1          predictions = torch.nn.functional.sigmoid(outputs.logits)  # Use sigmoid for multi-label classification\n",
      "    24                                           \n",
      "    25                                                   # Collect predictions on GPU\n",
      "    26      1250          0.0      0.0      0.0          results.append(predictions)\n",
      "    27                                           \n",
      "    28                                               # Concatenate all results on GPU\n",
      "    29         1          0.0      0.0      0.0      all_predictions = torch.cat(results, dim=0).cpu().numpy()\n",
      "    30                                           \n",
      "    31                                               # Convert to DataFrame\n",
      "    32        29          0.0      0.0      0.0      results_df = pd.DataFrame(all_predictions, columns=[id2label[i] for i in range(all_predictions.shape[1])])\n",
      "    33         1          0.0      0.0      0.0      results_df['index'] = range(len(df))\n",
      "    34                                           \n",
      "    35         1          0.0      0.0      0.0      elapsed_time = time.time() - start_time\n",
      "    36         1          0.0      0.0      0.0      messages_per_second = len(df) / elapsed_time\n",
      "    37                                           \n",
      "    38         1          0.0      0.0      0.0      return elapsed_time, messages_per_second, results_df"
     ]
    }
   ],
   "source": [
    "%lprun -u 1 -f sentiment_analysis_batched2 sentiment_analysis_batched2('onnx', model_id_onnx, df, field,batch_size, 0, file_name_onnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:16<00:00, 76.77it/s]\n"
     ]
    }
   ],
   "source": [
    "_,_,df5 = sentiment_analysis_batched3(model_id, df, field,batch_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Batch size 8): 100%|██████████| 1250/1250 [00:15<00:00, 80.96it/s]\n"
     ]
    }
   ],
   "source": [
    "_,_,df4 = sentiment_analysis_batched2(model_id, df, field,batch_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(batch size 8): 100%|██████████| 1250/1250 [00:35<00:00, 35.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 36.0961 s\n",
      "File: c:\\Users\\Joao\\repo\\gpu_benchmark_goemotions\\run_benchmark.py\n",
      "Function: sentiment_analysis_batched at line 56\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    56                                           def sentiment_analysis_batched(model_id, df, field_name, batch_size, gpu_id):\n",
      "    57         1          0.4      0.4      1.2      tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
      "    58         1          0.5      0.5      1.4      model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
      "    59         1          0.0      0.0      0.1      model.to(f'cuda:{gpu_id}')\n",
      "    60                                           \n",
      "    61                                               # Function to classify emotions of multiple texts in batched mode and return scores\n",
      "    62         1          0.0      0.0      0.0      def classify_texts(texts):\n",
      "    63                                                   inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
      "    64                                                   input_ids = inputs['input_ids'].to(f'cuda:{gpu_id}')\n",
      "    65                                                   attention_mask = inputs['attention_mask'].to(f'cuda:{gpu_id}')\n",
      "    66                                           \n",
      "    67                                                   with torch.no_grad():\n",
      "    68                                                       outputs = model(input_ids, attention_mask=attention_mask)\n",
      "    69                                                   probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
      "    70                                           \n",
      "    71                                                   # Process each item in the batch\n",
      "    72                                                   batch_results = []\n",
      "    73                                                   for prob in probabilities:\n",
      "    74                                                       result = {model.config.id2label[i]: prob_item.item() for i, prob_item in enumerate(prob)}\n",
      "    75                                                       batch_results.append(result)\n",
      "    76                                                       \n",
      "    77                                                   return batch_results\n",
      "    78                                           \n",
      "    79         1          0.0      0.0      0.0      start_time = time.time()\n",
      "    80                                           \n",
      "    81                                               # Placeholder for aggregated results\n",
      "    82         1          0.0      0.0      0.0      results = []\n",
      "    83                                           \n",
      "    84                                               # Iterate through the DataFrame in batches\n",
      "    85      1251          0.2      0.0      0.5      for start in tqdm(range(0, len(df), batch_size), desc=f\"(batch size {batch_size})\"):\n",
      "    86      1250          0.0      0.0      0.0          end = start + batch_size\n",
      "    87      1250          0.1      0.0      0.3          batch_texts = df[field_name].iloc[start:end].tolist()\n",
      "    88                                           \n",
      "    89                                                   # Apply classify_texts to the entire batch at once\n",
      "    90      1250         34.8      0.0     96.4          batch_results = classify_texts(batch_texts)\n",
      "    91                                                   \n",
      "    92                                                   # Assuming batch_results is a list of dictionaries\n",
      "    93      1250          0.0      0.0      0.0          results.extend(batch_results)\n",
      "    94                                           \n",
      "    95                                               # Merge the results with the original DataFrame\n",
      "    96         1          0.0      0.0      0.1      results_df = pd.DataFrame(results, index=df.index[:len(results)])\n",
      "    97         1          0.0      0.0      0.0      df = pd.concat([df, results_df], axis=1)\n",
      "    98                                           \n",
      "    99         1          0.0      0.0      0.0      elapsed_time = time.time() - start_time\n",
      "   100         1          0.0      0.0      0.0      messages_per_second = len(df) / elapsed_time\n",
      "   101                                           \n",
      "   102         1          0.0      0.0      0.0      return elapsed_time, messages_per_second"
     ]
    }
   ],
   "source": [
    "field = 'body'\n",
    "%lprun -u 1 -f sentiment_analysis_batched sentiment_analysis_batched(model_id, df, field,batch_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file onnx/model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "(batch size 8): 100%|██████████| 1250/1250 [00:27<00:00, 45.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1 s\n",
      "\n",
      "Total time: 30.3518 s\n",
      "File: c:\\Users\\Joao\\repo\\gpu_benchmark_goemotions\\run_benchmark.py\n",
      "Function: sentiment_analysis_onnx_batched at line 9\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     9                                           def sentiment_analysis_onnx_batched(model_id, file_name, df, field_name, batch_size, gpu_id):\n",
      "    10                                               \n",
      "    11         1          2.8      2.8      9.2      model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name, provider=\"CUDAExecutionProvider\", provider_options={'device_id': gpu_id})\n",
      "    12         1          0.2      0.2      0.7      tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
      "    13                                           \n",
      "    14                                               # Function to classify emotions of multiple texts in batched mode and return scores\n",
      "    15         1          0.0      0.0      0.0      def classify_texts(texts):\n",
      "    16                                                   # Tokenize the batch of texts\n",
      "    17                                                   inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
      "    18                                                   outputs = model(**inputs)\n",
      "    19                                           \n",
      "    20                                                   probabilities = torch.sigmoid(outputs.logits)\n",
      "    21                                                   labels = model.config.id2label  # Adjust if necessary\n",
      "    22                                                   \n",
      "    23                                                   # Process each item in the batch\n",
      "    24                                                   batch_results = []\n",
      "    25                                                   for prob in probabilities:\n",
      "    26                                                       result = {labels[i]: prob_item.item() for i, prob_item in enumerate(prob.squeeze())}\n",
      "    27                                                       batch_results.append(result)\n",
      "    28                                                       \n",
      "    29                                                   return batch_results\n",
      "    30                                           \n",
      "    31         1          0.0      0.0      0.0      start_time = time.time()\n",
      "    32                                           \n",
      "    33                                               # Placeholder for aggregated results\n",
      "    34         1          0.0      0.0      0.0      results = []\n",
      "    35                                               \n",
      "    36                                               # Iterate through the DataFrame in batches\n",
      "    37      1251          0.2      0.0      0.6      for start in tqdm(range(0, len(df), batch_size), desc=f\"(batch size {batch_size})\"):\n",
      "    38      1250          0.0      0.0      0.0          end = start + batch_size\n",
      "    39      1250          0.1      0.0      0.4          batch_texts = df[field_name].iloc[start:end].tolist()\n",
      "    40                                                   \n",
      "    41                                                   # Apply classify_text to the entire batch at once\n",
      "    42      1250         27.0      0.0     89.0          batch_results = classify_texts(batch_texts)\n",
      "    43                                                   \n",
      "    44                                                   # Assuming batch_results is a list of dictionaries\n",
      "    45      1250          0.0      0.0      0.0          results.extend(batch_results)\n",
      "    46                                                   \n",
      "    47                                               # Merge the results with the original DataFrame\n",
      "    48         1          0.0      0.0      0.1      results_df = pd.DataFrame(results, index=df.index[:len(results)])\n",
      "    49         1          0.0      0.0      0.0      df = pd.concat([df, results_df], axis=1)\n",
      "    50                                           \n",
      "    51         1          0.0      0.0      0.0      elapsed_time = time.time() - start_time\n",
      "    52         1          0.0      0.0      0.0      messages_per_second = len(df) / elapsed_time\n",
      "    53                                           \n",
      "    54         1          0.0      0.0      0.0      return elapsed_time, messages_per_second"
     ]
    }
   ],
   "source": [
    "field = 'body'\n",
    "batch_size = 8\n",
    "file_name_onnx = \"onnx/model.onnx\"\n",
    "gpu_id = 0\n",
    "%lprun -u 1 -f sentiment_analysis_onnx_batched sentiment_analysis_onnx_batched(model_id_onnx, file_name_onnx, df, field, batch_size, gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file onnx/model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n",
      "(batch size 8): 100%|██████████| 1250/1250 [00:28<00:00, 44.05it/s]\n"
     ]
    }
   ],
   "source": [
    "field = 'body'\n",
    "batch_size = 8\n",
    "file_name_onnx = \"onnx/model.onnx\"\n",
    "gpu_id = 0\n",
    "_,_,df2 = sentiment_analysis_onnx_batched(model_id_onnx, file_name_onnx, df, field, batch_size, gpu_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "import time\n",
    "\n",
    "\n",
    "def analyze_single_input_onnx(text):\n",
    "\n",
    "    model_id = \"SamLowe/roberta-base-go_emotions-onnx\"\n",
    "    file_name = \"onnx/model.onnx\"\n",
    "    gpu_id = 0\n",
    "\n",
    "    model = ORTModelForSequenceClassification.from_pretrained(model_id, file_name=file_name, provider=\"CUDAExecutionProvider\", provider_options={'device_id': gpu_id})\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    probabilities = torch.sigmoid(outputs.logits)\n",
    "    labels = model.config.id2label  # Adjust if necessary\n",
    "    result = {labels[i]: prob_item.item() for i, prob_item in enumerate(probabilities.squeeze())}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The ONNX file onnx/model.onnx is not a regular name used in optimum.onnxruntime, the ORTModel might not behave as expected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'admiration': 0.0011540877167135477,\n",
       " 'amusement': 0.0009100200841203332,\n",
       " 'anger': 0.0011203810572624207,\n",
       " 'annoyance': 0.008914163336157799,\n",
       " 'approval': 0.0285797081887722,\n",
       " 'caring': 0.003058207919821143,\n",
       " 'confusion': 0.03042701445519924,\n",
       " 'curiosity': 0.004150078631937504,\n",
       " 'desire': 0.0027006366290152073,\n",
       " 'disappointment': 0.0031804495956748724,\n",
       " 'disapproval': 0.018121996894478798,\n",
       " 'disgust': 0.0012776002986356616,\n",
       " 'embarrassment': 0.00030603265622630715,\n",
       " 'excitement': 0.00048009015154093504,\n",
       " 'fear': 0.0007079296628944576,\n",
       " 'gratitude': 0.0009287563734687865,\n",
       " 'grief': 0.0002476806694176048,\n",
       " 'joy': 0.0006493583787232637,\n",
       " 'love': 0.0008363121887668967,\n",
       " 'nervousness': 0.0003379562112968415,\n",
       " 'optimism': 0.010804506950080395,\n",
       " 'pride': 0.00018951600941363722,\n",
       " 'realization': 0.0179132167249918,\n",
       " 'relief': 0.00035339107853360474,\n",
       " 'remorse': 0.0004010939155705273,\n",
       " 'sadness': 0.0009339235257357359,\n",
       " 'surprise': 0.0004745300393551588,\n",
       " 'neutral': 0.9240930676460266}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"If you're targeting the JVM and ecosystem, you might prefer a language designed from the ground up to play within that ecosystem, rather than, say, something from the posix/C ecosystem ported to the JVM.\"\n",
    "analyze_single_input_onnx(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
